{
    "beaker": "2",
    "evaluators": [
        {
            "name": "Html",
            "plugin": "Html",
            "view": {
                "cm": {
                    "mode": "htmlmixed"
                }
            }
        },
        {
            "name": "JavaScript",
            "plugin": "JavaScript",
            "jsSetting2": "",
            "jsSetting1": "",
            "view": {
                "cm": {
                    "mode": "javascript",
                    "background": "#FFE0F0"
                }
            }
        },
        {
            "name": "IPython",
            "plugin": "IPython",
            "setup": "%matplotlib inline\nimport numpy\nimport matplotlib\nfrom matplotlib import pylab, mlab, pyplot\nnp = numpy\nplt = pyplot\nfrom IPython.display import display\nfrom IPython.core.pylabtools import figsize, getfigs\nfrom pylab import *\nfrom numpy import *\n",
            "view": {
                "cm": {
                    "mode": "python"
                }
            }
        }
    ],
    "cells": [
        {
            "id": "code7ZMmv7",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "import os",
                    "os.getcwd()",
                    "os.chdir('/Users/dspoka/Desktop/moscoso/nltk_childes/Hebrew') ",
                    "from nltk import data",
                    "from nltk.corpus.reader import CHILDESCorpusReader",
                    "from nltk.probability import FreqDist",
                    "import entropies7.entropies7 as Ent",
                    "from numpy import *",
                    "import scipy as sp",
                    "import numpy as np",
                    "import re",
                    "import os as os",
                    "",
                    "%matplotlib inline",
                    "from pandas import *",
                    "import matplotlib.pyplot as plt",
                    "from matplotlib.font_manager import FontProperties",
                    "",
                    "def convert_age_to_days(age):",
                    "    # Takes P[0-9]Y[1-12]M[1-31]D",
                    "    # Outputs integer number of days",
                    "    parsed_age = re.split('P|Y|M|D',age)",
                    "    years = parsed_age[1]",
                    "    months = parsed_age[2]",
                    "    days = parsed_age[3]",
                    "    if days == '':",
                    "        days = 0",
                    "    return int(years) * 365 + int(months) * 30 + int(days)",
                    ""
                ]
            },
            "output": {
                "state": {},
                "selectedType": "Hidden",
                "pluginName": "IPython",
                "shellId": "2AFA94D3B37A45E9B56AF14B18DB0EB6",
                "elapsedTime": 333
            },
            "evaluatorReader": true,
            "lineCount": 29
        },
        {
            "id": "codeVkbQ0l",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "def get_files(corpus):",
                    "    # Inputs the name of the corpus ",
                    "    # Outputs:",
                    "    # corpus_root: string local path to corpora /.../nltk_data/corpora/childes/[corpus]",
                    "    # heb: CHILDESCorpusReader tool",
                    "    # files: lsit of name of files in corpus_root",
                    "    # data_directory: string location of future output data",
                    "    corpus_root = data.find('corpora/childes/Hebrew-MOR')",
                    "    heb = CHILDESCorpusReader(corpus_root, 'BermanLong/.*xml')",
                    "#     #CHANGE THIS",
                    "#     if(corpus_root == \"/Users/dspoka/nltk_data/corpora/childes/English-UK-MOR\"):",
                    "#         data_directory = \"/Users/dspoka/Desktop/moscoso/nltk_childes/NLTKCHILDES/\"",
                    "#     else:",
                    "    data_directory = \"/Users/dspoka/Desktop/moscoso/nltk_childes/Hebrew/data_directory/\"",
                    "    files = heb.fileids()",
                    "    return corpus_root, heb, files, data_directory",
                    "# %timeit files = get_files('childes')",
                    "corpus_root, heb, files, data_directory = get_files('manchester')",
                    "nmmfile = data_directory + \"morph-heb.csv\"",
                    "nmsfile = data_directory + \"syntax-heb.csv\"",
                    "# childes has 804 files"
                ]
            },
            "output": {
                "state": {},
                "selectedType": "BeakerDisplay",
                "pluginName": "IPython",
                "shellId": "2AFA94D3B37A45E9B56AF14B18DB0EB6",
                "elapsedTime": 317
            },
            "evaluatorReader": true,
            "lineCount": 21
        },
        {
            "id": "code4H0LWX",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "def compare(f1, f2):",
                    "    # Inputs two files",
                    "    # Outputs -1 if the child is younger in f1, 1 if child older in f1, 0 if same age",
                    "    age1 = convert_age_to_days(heb.age(f1)[0])",
                    "    age2 = convert_age_to_days(heb.age(f2)[0])",
                    "    if age1 < age2:",
                    "        return -1",
                    "    elif age2 < age1:",
                    "        return 1",
                    "    else:",
                    "        return 0",
                    "   ",
                    "def group_files_by_child_age(files):",
                    "    # Inputs list of all filenames",
                    "    # Outputs dictionary with keys child names and values list of file names (sorted by age) corresponding to child",
                    "    child_files_dict = {}",
                    "    for f in files:",
                    "        child = f.split(\"/\")[1]",
                    "        if child_files_dict.has_key(child):",
                    "            child_files_dict[child] += [f]",
                    "        else:",
                    "            child_files_dict[child] = [f]",
                    "    for child in child_files_dict.keys():",
                    "        child_files_dict[child] = sorted(child_files_dict[child], cmp=compare)",
                    "    return child_files_dict",
                    "",
                    "# %timeit child_files_dict = group_files_by_child_age(files)",
                    "# 30s to run",
                    "child_files_dict = group_files_by_child_age(files)"
                ]
            },
            "output": {
                "state": {},
                "selectedType": "BeakerDisplay",
                "pluginName": "IPython",
                "shellId": "2AFA94D3B37A45E9B56AF14B18DB0EB6",
                "elapsedTime": 28616
            },
            "evaluatorReader": true,
            "lineCount": 29
        },
        {
            "id": "codeSsa4Zo",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "child_files_dict['hagar'][0]"
                ]
            },
            "output": {
                "state": {},
                "result": "<div class=\"output_subarea output_text\"><pre>u'BermanLong/hagar/hag107a.xml'</pre></div>",
                "selectedType": "Html",
                "pluginName": "IPython",
                "shellId": "2AFA94D3B37A45E9B56AF14B18DB0EB6",
                "elapsedTime": 310,
                "dataresult": "BermanLong/hagar/hag107a.xml"
            },
            "evaluatorReader": true,
            "lineCount": 1
        },
        {
            "id": "codezCDEdg",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "# \"CHI, MOT, FAT, RAY, GMO, GFA, GFO, GRM, GRF, INV, NIC, SEF, TAL, VIS, YOX, YAE\"",
                    "f = 'BermanLong/hagar/hag107a.xml'",
                    "child_words = heb.words(f,speaker=\"CHI\",replace=True)",
                    "child_stems = [w.split(\"-\")[0] for w in heb.words(f,speaker=\"CHI\",replace=True,stem=True)]",
                    "for unique in set(child_words):",
                    "    print unique",
                    "#NOTE: NOT ALL FILES HAVE MOTHER AS A SPEAKER - WILL YIELD 0'S AND NAN'S"
                ]
            },
            "output": {
                "state": {},
                "result": {
                    "type": "Results",
                    "outputdata": [
                        {
                            "type": "out",
                            "value": "hīne\nʔābaʔ\nqaṭān\nyōfi\nʕeynāyim\nʕitōn\nze\nlalalala\nlēxem\ndēlet\ngag\nMimixa\nmiḳnasāyim\nʕod_pāʕam\nšafān\nlalala\nqafē\ntaq\nʔeš\nle\nladow\nšar\nrocā\nʔavāl\ntiq\nyam\nxam\nyad\nmitgalēax\nnaʕalāyim\nlašīr\nnadnēd\nsipūr\nʔor\nxxx\nkōvaʕ\nʔīmaʔ\nda\nyādi\nloʔ\nʕod\nHagari\ne\nmoa\nha\negen\nma\nyēled\no\nlemāʕla\nhigāʕti\ngalgāl\nʔaryē\nšamoa\nlesapēr\n"
                        }
                    ]
                },
                "selectedType": "Results",
                "pluginName": "IPython",
                "shellId": "2AFA94D3B37A45E9B56AF14B18DB0EB6",
                "elapsedTime": 322,
                "dataresult": "577"
            },
            "evaluatorReader": true,
            "lineCount": 7
        },
        {
            "id": "codeVEEjSo",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "def make_dir(directory):",
                    "    # Inputs a directory path",
                    "    # Creates a directory path at the input path if it does not already exist",
                    "    if not os.path.exists(directory):",
                    "        os.makedirs(directory)",
                    "",
                    "def output_words_to_files(child_files_dict):",
                    "    # Inputs dictionary from children to their files",
                    "    # Writes (for every child and every file) a list of words (with age on top) corresponding to that child and file",
                    "    # \"/[child_name]/[speaker][_stem?]/[child_name][0-9][a-c]\" where terminating characters come from original filename",
                    "    for child in child_files_dict.keys():",
                    "        directory = data_directory + child + \"/\"",
                    "        make_dir(directory)",
                    "        for f in child_files_dict[child]:",
                    "            child_dir = directory + \"child/\"",
                    "            mother_dir = directory + \"mother/\"",
                    "            child_stem_dir = directory + \"child_stem/\"",
                    "            mother_stem_dir = directory + \"mother_stem/\"",
                    "            file_name = (str(f)[11:-4].split(\"/\"))[1]",
                    "            make_dir(child_dir)",
                    "            make_dir(mother_dir)",
                    "            make_dir(child_stem_dir)",
                    "            make_dir(mother_stem_dir)",
                    "            write_list_to_file(f, child_dir + file_name, False, \"CHI\")",
                    "            write_list_to_file(f, mother_dir + file_name, False, \"MOT\")",
                    "            write_list_to_file(f, child_stem_dir + file_name, True, \"CHI\")",
                    "            write_list_to_file(f, mother_stem_dir + file_name, True, \"MOT\") ",
                    "            ",
                    "def write_sentences_to_file(f, file_path, speaker):",
                    "    # Inputs a file f, and out file_path file_path, and speaker (MOT or CHI)",
                    "    # Writes (to the file_path) sentences (one per line) used in f by speaker",
                    "    fout = open(file_path, \"w\")",
                    "    sentences = heb.sents(f, speaker=speaker,replace=True)",
                    "    for sentence in sentences:",
                    "        fout.write(\" \".join([word.encode('utf8') for word in sentence if word != \"xxx\"]) + \"\\n\")",
                    "    fout.close()",
                    "            ",
                    "def output_sentences_to_files(child_files_dict):",
                    "    # Inputs dictionary from children to their files",
                    "    # Writes (for every child and every file) a list of sentences corresponding to that child and file",
                    "    # \"/[child_name]/[speaker]_sentences/[child_name][0-9][a-c]\" where terminating characters come from original filename",
                    "    for child in child_files_dict.keys():",
                    "        directory = data_directory + child + \"/\"",
                    "        make_dir(directory)",
                    "        for f in child_files_dict[child]:",
                    "            child_sentence_dir = directory + \"child_sentences/\"",
                    "            mother_sentence_dir = directory + \"mother_sentences/\"",
                    "            file_name = (str(f)[11:-4].split(\"/\"))[1]",
                    "            make_dir(child_sentence_dir)",
                    "            make_dir(mother_sentence_dir)",
                    "            write_sentences_to_file(f, child_sentence_dir + file_name, \"CHI\")",
                    "            write_sentences_to_file(f, mother_sentence_dir + file_name, \"MOT\")",
                    "",
                    "def concatenate_tuples(tuple):",
                    "    return tuple[0]+str(tuple[1])",
                    "            ",
                    "def write_list_to_file(f, file_path, use_stem, speaker):",
                    "    # Inputs a single child's file name, an out file_path name, a use_stem flag, and speaker (MOT or CHI)",
                    "    # Writes (to the file file_path) the age of the child in the file f and then the list of all words (or stems) used in f, newline separated",
                    "    fout = open(file_path, \"w\")",
                    "    if(use_stem == False):",
                    "        words = map(concatenate_tuples, heb.tagged_words(f,speaker=speaker, replace=True))",
                    "#         words = heb.words(f,speaker=speaker,replace=True)",
                    "        fout.write(str(convert_age_to_days(heb.age(f)[0])) + \"\\n\")",
                    "        for word in words:",
                    "            if(word != \"xxx\"):",
                    "                fout.write(word.encode('utf8') + \"\\n\")",
                    "    else:",
                    "        words = [w.split(\"-\")[0] for w in heb.words(f,speaker=speaker,replace=True,stem=True)]",
                    "        fout.write(str(convert_age_to_days(heb.age(f)[0])) + \"\\n\")",
                    "        for word in words:",
                    "            if(word != \"xxx\"):",
                    "                fout.write(word.encode('utf8') + \"\\n\")",
                    "    fout.close()",
                    "    ",
                    "# <10 min to run",
                    "# small_dict = {}",
                    "# small_dict['hagar'] = [child_files_dict['hagar'][0]]",
                    "# small_dict['hagar'] += [child_files_dict['hagar'][1]]",
                    "# # print child_files_dict['hagar']",
                    "# print small_dict",
                    "# output_words_to_files(small_dict)",
                    "# output_sentences_to_files(small_dict)",
                    "output_words_to_files(child_files_dict)",
                    "output_sentences_to_files(child_files_dict)",
                    ""
                ]
            },
            "output": {
                "state": {},
                "selectedType": "Results",
                "pluginName": "IPython",
                "shellId": "2AFA94D3B37A45E9B56AF14B18DB0EB6",
                "elapsedTime": 229895,
                "dataresult": [
                    "BermanLong/hagar/hag107a.xml",
                    "BermanLong/hagar/hag107b.xml",
                    "BermanLong/hagar/hag107c.xml",
                    "BermanLong/hagar/hag107d.xml",
                    "BermanLong/hagar/hag107e.xml",
                    "BermanLong/hagar/hag107f.xml",
                    "BermanLong/hagar/hag107g.xml",
                    "BermanLong/hagar/hag107h.xml",
                    "BermanLong/hagar/hag107i.xml",
                    "BermanLong/hagar/hag108a.xml",
                    "BermanLong/hagar/hag108b.xml",
                    "BermanLong/hagar/hag108c.xml",
                    "BermanLong/hagar/hag108d.xml",
                    "BermanLong/hagar/hag108e.xml",
                    "BermanLong/hagar/hag108f.xml",
                    "BermanLong/hagar/hag108g.xml",
                    "BermanLong/hagar/hag108h.xml",
                    "BermanLong/hagar/hag108i.xml",
                    "BermanLong/hagar/hag109a.xml",
                    "BermanLong/hagar/hag109b.xml",
                    "BermanLong/hagar/hag109c.xml",
                    "BermanLong/hagar/hag109d.xml",
                    "BermanLong/hagar/hag109e.xml",
                    "BermanLong/hagar/hag109f.xml",
                    "BermanLong/hagar/hag108j.xml",
                    "BermanLong/hagar/hag109g.xml",
                    "BermanLong/hagar/hag109h.xml",
                    "BermanLong/hagar/hag109i.xml",
                    "BermanLong/hagar/hag109j.xml",
                    "BermanLong/hagar/hag109k.xml",
                    "BermanLong/hagar/hag110a.xml",
                    "BermanLong/hagar/hag110b.xml",
                    "BermanLong/hagar/hag110c.xml",
                    "BermanLong/hagar/hag110d.xml",
                    "BermanLong/hagar/hag110e.xml",
                    "BermanLong/hagar/hag110f.xml",
                    "BermanLong/hagar/hag110g.xml",
                    "BermanLong/hagar/hag110h.xml",
                    "BermanLong/hagar/hag110i.xml",
                    "BermanLong/hagar/hag110j.xml",
                    "BermanLong/hagar/hag110k.xml",
                    "BermanLong/hagar/hag110l.xml",
                    "BermanLong/hagar/hag110m.xml",
                    "BermanLong/hagar/hag110n.xml",
                    "BermanLong/hagar/hag110o.xml",
                    "BermanLong/hagar/hag110p1.xml",
                    "BermanLong/hagar/hag110p2.xml",
                    "BermanLong/hagar/hag110q.xml",
                    "BermanLong/hagar/hag111a.xml",
                    "BermanLong/hagar/hag111b.xml",
                    "BermanLong/hagar/hag111c.xml",
                    "BermanLong/hagar/hag111d.xml",
                    "BermanLong/hagar/hag111e.xml",
                    "BermanLong/hagar/hag111f.xml",
                    "BermanLong/hagar/hag111g.xml",
                    "BermanLong/hagar/hag111h.xml",
                    "BermanLong/hagar/hag111i.xml",
                    "BermanLong/hagar/hag111j1.xml",
                    "BermanLong/hagar/hag111j2.xml",
                    "BermanLong/hagar/hag111k.xml",
                    "BermanLong/hagar/hag200a.xml",
                    "BermanLong/hagar/hag200b.xml",
                    "BermanLong/hagar/hag200c.xml",
                    "BermanLong/hagar/hag200d.xml",
                    "BermanLong/hagar/hag201a.xml",
                    "BermanLong/hagar/hag201b.xml",
                    "BermanLong/hagar/hag201c1.xml",
                    "BermanLong/hagar/hag201c2.xml",
                    "BermanLong/hagar/hag201d.xml",
                    "BermanLong/hagar/hag201e.xml",
                    "BermanLong/hagar/hag201f.xml",
                    "BermanLong/hagar/hag201g.xml",
                    "BermanLong/hagar/hag201h.xml",
                    "BermanLong/hagar/hag201i.xml",
                    "BermanLong/hagar/hag201j.xml",
                    "BermanLong/hagar/hag201k.xml",
                    "BermanLong/hagar/hag201l.xml",
                    "BermanLong/hagar/hag201m1.xml",
                    "BermanLong/hagar/hag201m2.xml",
                    "BermanLong/hagar/hag202a.xml",
                    "BermanLong/hagar/hag202b.xml",
                    "BermanLong/hagar/hag202c.xml",
                    "BermanLong/hagar/hag202d.xml",
                    "BermanLong/hagar/hag202e.xml",
                    "BermanLong/hagar/hag202f.xml",
                    "BermanLong/hagar/hag202g.xml",
                    "BermanLong/hagar/hag202h.xml",
                    "BermanLong/hagar/hag202i.xml",
                    "BermanLong/hagar/hag202j.xml",
                    "BermanLong/hagar/hag203a.xml",
                    "BermanLong/hagar/hag203b.xml",
                    "BermanLong/hagar/hag203c.xml",
                    "BermanLong/hagar/hag203d.xml",
                    "BermanLong/hagar/hag203e.xml",
                    "BermanLong/hagar/hag204a.xml",
                    "BermanLong/hagar/hag204b.xml",
                    "BermanLong/hagar/hag204c.xml",
                    "BermanLong/hagar/hag204d.xml",
                    "BermanLong/hagar/hag204e.xml",
                    "BermanLong/hagar/hag204f.xml",
                    "BermanLong/hagar/hag204g.xml",
                    "BermanLong/hagar/hag204h.xml",
                    "BermanLong/hagar/hag204i.xml",
                    "BermanLong/hagar/hag204j.xml",
                    "BermanLong/hagar/hag204k.xml",
                    "BermanLong/hagar/hag204l.xml",
                    "BermanLong/hagar/hag205a.xml",
                    "BermanLong/hagar/hag205b.xml",
                    "BermanLong/hagar/hag205c.xml",
                    "BermanLong/hagar/hag206a.xml",
                    "BermanLong/hagar/hag206b.xml",
                    "BermanLong/hagar/hag206c.xml",
                    "BermanLong/hagar/hag206e.xml",
                    "BermanLong/hagar/hag206f.xml",
                    "BermanLong/hagar/hag207a.xml",
                    "BermanLong/hagar/hag207b.xml",
                    "BermanLong/hagar/hag207c.xml",
                    "BermanLong/hagar/hag207d.xml",
                    "BermanLong/hagar/hag207e.xml",
                    "BermanLong/hagar/hag207g.xml",
                    "BermanLong/hagar/hag208a.xml",
                    "BermanLong/hagar/hag208b.xml",
                    "BermanLong/hagar/hag208c.xml",
                    "BermanLong/hagar/hag208d.xml",
                    "BermanLong/hagar/hag208e.xml",
                    "BermanLong/hagar/hag208f.xml",
                    "BermanLong/hagar/hag208g.xml",
                    "BermanLong/hagar/hag209a.xml",
                    "BermanLong/hagar/hag209b.xml",
                    "BermanLong/hagar/hag209c.xml",
                    "BermanLong/hagar/hag209d.xml",
                    "BermanLong/hagar/hag210a.xml",
                    "BermanLong/hagar/hag211a.xml",
                    "BermanLong/hagar/hag303a-gold.xml"
                ],
                "result": {
                    "type": "Results",
                    "outputdata": [
                        {
                            "type": "out",
                            "value": "(u'\\u0294af_pa\\u0304\\u0295am', 'adv')\n(u'\\u0294af_pa\\u0304\\u0295am', 'adv')\n(u'hu\\u0294', 'pro:person')\n(u'tami\\u0304d', 'adv')\n(u'haya\\u0304', 'cop')\n(u'qat\\u0323a\\u0304n', 'adj')\n(u'mime\\u0304k\\u0323', 'prep:pro')\n('we', 'conj')\n(u'hu\\u0294', 'pro:person')\n(u'yis\\u030ca\\u0294e\\u0304r', 'v')\n(u'qat\\u0323a\\u0304n', 'adj')\n(u'mime\\u0304k\\u0323', 'prep:pro')\n(u'\\u0294e\\u0304yze', 'que')\n(u'mu\\u0304ziqa', 'n')\n(u'yafa\\u0304', 'adj')\n(u'Nica\\u0304n', 'n:prop')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n('ma', 'que')\n(u'\\u0295as\\u0323i\\u0304ti', 'v')\n('lo', 'prep:pro')\n(u'tagi\\u0304di', 'v')\n(u'xemdi\\u0304qa', 'n:dim')\n(u'yeladi\\u0304m', 'n')\n(u'yeladi\\u0304m', 'n')\n(u'yeladi\\u0304m', 'n')\n(u'yeladi\\u0304m', 'n')\n(u'yeladi\\u0304m', 'n')\n(u'yeladi\\u0304m', 'n')\n(u'yeladi\\u0304m', 'n')\n('ma', 'que')\n(u's\\u030cum_dava\\u0304r', 'n:indef')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'zok\\u0323e\\u0304ret', 'part')\n(u's\\u030ce', 'conj:subor')\n(u's\\u030ca\\u0294a\\u0304lt', 'v')\n(u'\\u0294oti\\u0304', 'acc:pro')\n(u'\\u0294eyk\\u0323', 'que')\n(u'\\u0295os\\u0323i\\u0304m', 'part')\n(u'calaxo\\u0304t', 'n')\n(u'\\u0294at', 'pro:person')\n(u'zok\\u0323e\\u0304ret', 'part')\n('ma', 'que')\n(u'\\u0294ama\\u0304rti', 'v')\n(u'lak\\u0323', 'prep:pro')\n(u'\\u0294e\\u0304yze', 'que')\n(u'ca\\u0295acu\\u0295i\\u0304m', 'n')\n(u'tistakli\\u0304', 'v')\n(u'qo\\u0304dem', 'adv')\n('me', 'prep')\n(u'\\u0294e\\u0304yze', 'que')\n(u'xo\\u0304mer', 'n')\n('ze', 'pro:dem')\n(u'\\u0295as\\u0323u\\u0304y', 'adj')\n(u'lo\\u0294', 'co')\n('be', 'prep')\n(u'\\u0294e\\u0304yze', 'que')\n(u'ce\\u0304va\\u0295', 'n')\n('ze', 'pro:dem')\n(u'nak\\u0323o\\u0304n', 'co')\n('ze', 'pro:dem')\n(u'ce\\u0304va\\u0295', 'n')\n(u'kato\\u0304m', 'adj')\n('ze', 'pro:dem')\n(u'\\u0294ava\\u0304l', 'conj')\n('ze', 'pro:dem')\n('mi', 'prep')\n(u'zk\\u0323uk\\u0323i\\u0304t', 'n')\n('ma', 'que')\n('ze', 'pro:dem')\n(u'nak\\u0323o\\u0304n', 'co')\n('ze', 'pro:dem')\n('mi', 'prep')\n(u'pla\\u0304st\\u0323iq', 'n')\n(u'pla\\u0304st\\u0323iq', 'n')\n('ze', 'pro:dem')\n(u'xo\\u0304mer', 'n')\n('we', 'conj')\n(u'zk\\u0323uk\\u0323i\\u0304t', 'n')\n('ze', 'pro:dem')\n(u'xo\\u0304mer', 'n')\n('we', 'conj')\n(u'\\u0295ec', 'n')\n('ze', 'pro:dem')\n(u'xo\\u0304mer', 'n')\n('ma', 'que')\n(u'\\u0295as\\u0323u\\u0304y', 'adj')\n('me', 'prep')\n('ha', 'det')\n(u'\\u0295ec', 'n')\n('mi', 'prep')\n('ma', 'que')\n(u'\\u0295as\\u0323uya\\u0304', 'adj')\n('ha', 'det')\n(u'mit\\u0323a\\u0304', 'n')\n(u's\\u030cela\\u0304k\\u0323', 'prep:pro')\n(u'tir\\u0294i\\u0304', 'v')\n(u'\\u0294e\\u0304yze', 'que')\n(u'xo\\u0304mer', 'n')\n('ze', 'pro:dem')\n('mi', 'prep')\n('ze', 'pro:dem')\n('mi', 'prep')\n(u'pla\\u0304st\\u0323iq', 'n')\n(u'lo\\u0294', 'co')\n('ze', 'pro:dem')\n('mi', 'prep')\n(u'zk\\u0323uk\\u0323i\\u0304t', 'n')\n(u'lo\\u0294', 'co')\n('ha', 'det')\n(u'mit\\u0323a\\u0304', 'n')\n(u's\\u030cela\\u0304k\\u0323', 'prep:pro')\n(u'\\u0295as\\u0323uya\\u0304', 'adj')\n('me', 'prep')\n(u'\\u0295ec', 'n')\n(u'\\u0295ec', 'n')\n('ze', 'pro:dem')\n(u'xo\\u0304mer', 'n')\n('we', 'conj')\n(u'pla\\u0304st\\u0323iq', 'n')\n('ze', 'pro:dem')\n(u'xo\\u0304mer', 'n')\n('we', 'conj')\n(u'zk\\u0323uk\\u0323i\\u0304t', 'n')\n('ze', 'pro:dem')\n(u'xo\\u0304mer', 'n')\n('mi', 'prep')\n('ma', 'que')\n(u'\\u0295as\\u0323u\\u0304y', 'adj')\n('ha', 'det')\n(u'ca\\u0295acu\\u0304a\\u0295', 'n')\n('ha_ze', 'pro:dem')\n(u'hu\\u0294', 'pro:person')\n('mi', 'prep')\n(u'zk\\u0323uk\\u0323i\\u0304t', 'n')\n(u'tistakli\\u0304', 'v')\n(u'lo\\u0294', 'co')\n(u'\\u0294im', 'conj:subor')\n(u'\\u0294im', 'conj:subor')\n(u'hu\\u0294', 'pro:person')\n(u'haya\\u0304', 'cop')\n('mi', 'prep')\n(u'zk\\u0323uk\\u0323i\\u0304t', 'n')\n('we', 'conj')\n(u'Nica\\u0304n', 'n:prop')\n(u'haya\\u0304', 'cop')\n(u'zore\\u0304q', 'part')\n(u'\\u0294oto\\u0304', 'acc:pro')\n(u'kmo_s\\u030ce', 'conj:subor')\n(u'hu\\u0294', 'pro:person')\n(u'zara\\u0304q', 'v')\n(u'hu\\u0294', 'pro:person')\n(u'bandi\\u0304t\\u0323', 'adj')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'\\u0294ata\\u0304', 'pro:person')\n('ha', 'det')\n('ha', 'det')\n('ha', 'det')\n('ha', 'det')\n('ha', 'det')\n('ha', 'det')\n('ha', 'det')\n('ha', 'det')\n('we', 'conj')\n(u'bifni\\u0304m', 'adv')\n(u'yes\\u030c', 'exs')\n(u'xelqe\\u0304y', 'n')\n(u'mate\\u0304k\\u0323et', 'n')\n('ma', 'que')\n(u'\\u0295os\\u0323i\\u0304m', 'part')\n('be', 'prep')\n('ma', 'que')\n(u'ca\\u0295acu\\u0304a\\u0295', 'n')\n(u'ca\\u0295acu\\u0304a\\u0295', 'n')\n(u'ca\\u0295acu\\u0304a\\u0295', 'n')\n(u'ca\\u0295acu\\u0304a\\u0295', 'n')\n(u'ca\\u0295acu\\u0304a\\u0295', 'n')\n(u'\\u0294ahhah', 'voc')\n('ma', 'que')\n(u's\\u030ce', 'conj:subor')\n(u'\\u0294at', 'pro:person')\n(u'maxziqa\\u0304', 'part')\n('ze', 'pro:dem')\n(u'kol_mine\\u0304y', 'qn')\n(u'xalaqi\\u0304m', 'n')\n(u's\\u030ce', 'conj:subor')\n(u'\\u0294efs\\u030ca\\u0304r', 'adv')\n(u'livno\\u0304t', 'v')\n('me', 'prep')\n('hem', 'pro:person')\n(u'curo\\u0304t', 'n')\n('ze', 'pro:dem')\n(u'nir\\u0294e\\u0304', 'part')\n('kmo', 'prep')\n('pil', 'n')\n(u'yo\\u0304fi', 'co')\n(u'yafe\\u0304_me\\u0294o\\u0304d', 'co')\n(u'Li\\u0294o\\u0304r', 'n:prop')\n('kmo', 'prep')\n('ma', 'que')\n('ze', 'pro:dem')\n(u'nir\\u0294e\\u0304', 'part')\n(u'\\u0295ak\\u0323s\\u030ca\\u0304yw', 'adv')\n('ma', 'que')\n(u'qara\\u0304', 'v')\n('ze', 'pro:dem')\n(u'nir\\u0294a\\u0304', 'part')\n('kmo', 'prep')\n('ma', 'que')\n('ze', 'pro:dem')\n(u'nir\\u0294a\\u0304', 'part')\n('kmo', 'prep')\n(u'tofe\\u0304s\\u0323', 'part')\n('ha', 'det')\n(u'\\u0294af', 'n')\n(u's\\u030celo\\u0304', 'prep:pro')\n(u'nitpa\\u0304s', 'part')\n('ha', 'det')\n(u'xe\\u0304deq', 'n')\n(u's\\u030cel', 'prep')\n('ha', 'det')\n('pil', 'n')\n(u'nitpa\\u0304s', 'v')\n(u'hu\\u0294', 'pro:person')\n(u'lo\\u0294', 'neg')\n(u'\\u0294ome\\u0304r', 'part')\n(u'lak\\u0323', 'prep:pro')\n(u'ha\\u0304\\u0294lo', 'co')\n(u'hu\\u0294', 'pro:person')\n(u'roce\\u0304', 'part')\n(u'litpo\\u0304s', 'v')\n(u'hako\\u0304l', 'pro:indef')\n(u'be\\u0304t\\u0323ax', 'adv')\n(u's\\u030ce', 'conj:subor')\n(u'hu\\u0294', 'pro:person')\n(u'bese\\u0304der', 'co')\n('kol', 'qn')\n('ha', 'det')\n(u'pili\\u0304m', 'n')\n(u'bese\\u0304der', 'co')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'lo\\u0294', 'co')\n('ze', 'pro:dem')\n('raq', 'adv')\n(u's\\u030cela\\u0304k\\u0323', 'prep:pro')\n(u'\\u0294at', 'pro:person')\n(u'\\u0295ada\\u0304yin', 'adv')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'lesape\\u0304r', 'v')\n('gam', 'qn')\n(u'\\u0295ak\\u0323s\\u030ca\\u0304yw', 'adv')\n(u'yes\\u030c', 'exs')\n(u'dvari\\u0304m', 'n')\n(u's\\u030ce', 'conj:subor')\n(u'\\u0294at', 'pro:person')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'lesape\\u0304r', 'v')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294ava\\u0304l', 'conj')\n(u're\\u0304ga\\u0295', 'n')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'lo\\u0294', 'neg')\n(u's\\u030coma\\u0304\\u0295at', 'part')\n(u'\\u0294ota\\u0304k\\u0323', 'acc:pro')\n(u'tatxi\\u0304li', 'v')\n(u'me_hatxala\\u0304', 'adv')\n('me', 'prep')\n(u'\\u0294e\\u0304yfo_s\\u030ce', 'conj:subor')\n(u'\\u0294at', 'pro:person')\n(u'roca\\u0304', 'part')\n(u'qore\\u0304\\u0294t', 'part')\n(u'mito\\u0304k\\u0323', 'prep')\n(u'se\\u0304fer', 'n')\n(u'\\u0294at', 'pro:person')\n(u'qore\\u0304\\u0294t', 'part')\n(u'mito\\u0304k\\u0323', 'prep')\n(u'se\\u0304fer', 'n')\n(u'\\u0294at', 'pro:person')\n(u'mistake\\u0304let', 'part')\n('ba', 'prep')\n(u'tmuno\\u0304t', 'n')\n('we', 'conj')\n(u'\\u0294at', 'pro:person')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'lesape\\u0304r', 'v')\n(u'\\u0294et', 'acc')\n('ma', 'que')\n(u's\\u030ce', 'conj:subor')\n(u'\\u0294at', 'pro:person')\n(u'ro\\u0294a\\u0304', 'part')\n('ha', 'det')\n(u'cipo\\u0304r', 'n')\n(u'hitpatxa\\u0304', 'v')\n('mi', 'prep')\n('ma', 'que')\n(u'zo\\u0294t', 'pro:dem')\n(u'xalali\\u0304t', 'n')\n('we', 'conj')\n('ha', 'det')\n(u'cipo\\u0304r', 'n')\n(u'lo\\u0294', 'neg')\n(u'hitpatxa\\u0304', 'v')\n('mi', 'prep')\n(u'xalali\\u0304t', 'n')\n('mi', 'prep')\n('ma', 'que')\n(u'hitpatxa\\u0304', 'v')\n('ha', 'det')\n(u'cipo\\u0304r', 'n')\n('ze', 'pro:dem')\n(u'xalali\\u0304t', 'n')\n(u'zo\\u0294t', 'pro:dem')\n(u'xalali\\u0304t', 'n')\n(u'Li\\u0294o\\u0304r', 'n:prop')\n(u'\\u0294er', 'unk')\n(u'lo\\u0294', 'co')\n('ze', 'pro:dem')\n('ba', 'prep')\n(u's\\u030cama\\u0304yim', 'n')\n('mi', 'prep')\n('ma', 'que')\n(u'hitpatxa\\u0304', 'v')\n('ha', 'det')\n(u'cipo\\u0304r', 'n')\n(u'Li\\u0294o\\u0304r', 'n:prop')\n('mi', 'prep')\n(u's\\u030cum_dava\\u0304r', 'n:indef')\n(u's\\u030cum_dava\\u0304r', 'n:indef')\n(u'lo\\u0294', 'neg')\n(u'mitpate\\u0304ax', 'part')\n('mi', 'prep')\n(u's\\u030cum_dava\\u0304r', 'n:indef')\n('kol', 'qn')\n(u'dava\\u0304r', 'n')\n(u'mitpate\\u0304ax', 'part')\n('mi', 'prep')\n(u'ma\\u0304s\\u030cehu', 'pro:indef')\n(u'nak\\u0323o\\u0304n', 'adv')\n(u'nak\\u0323o\\u0304n', 'adv')\n(u'nak\\u0323o\\u0304n', 'adv')\n(u'yes\\u030c', 'exs')\n(u'hako\\u0304l', 'pro:indef')\n(u'beto\\u0304k\\u0323', 'prep')\n('ha', 'det')\n(u'xalali\\u0304t', 'n')\n(u'\\u0294ahha', 'voc')\n(u'tagi\\u0304di', 'v')\n(u'le\\u0294a\\u0304n', 'que')\n(u't\\u0323asi\\u0304m', 'part')\n('ba', 'prep')\n(u'xalali\\u0304t', 'n')\n(u'tagi\\u0304di', 'v')\n(u'psante\\u0304r', 'n')\n(u'nak\\u0323o\\u0304n', 'co')\n('ze', 'pro:dem')\n(u'dome\\u0304', 'adj')\n('le', 'prep')\n(u'xanukiya\\u0304', 'n')\n(u'\\u0294ava\\u0304l', 'conj')\n('ze', 'pro:dem')\n(u'lo\\u0294', 'neg')\n(u'xanukiya\\u0304', 'n')\n('ze', 'pro:dem')\n(u'menora\\u0304', 'n')\n('ze', 'pro:dem')\n(u'menora\\u0304', 'n')\n(u's\\u030ce', 'conj:subor')\n('ken', 'co')\n('ze', 'pro:dem')\n(u'me\\u0294o\\u0304d', 'adv')\n(u'dome\\u0304', 'adj')\n('le', 'prep')\n(u'xanukiya\\u0304', 'n')\n(u'\\u0294ava\\u0304l', 'conj')\n(u'tir\\u0294i\\u0304', 'v')\n(u'\\u0294eyn', 'exs')\n('lah', 'prep:pro')\n(u'qani\\u0304m', 'n')\n(u'kmo_s\\u030ce', 'conj:subor')\n(u'yes\\u030c', 'exs')\n('la', 'prep')\n(u'xanukiya\\u0304', 'n')\n(u'yes\\u030c', 'exs')\n('lah', 'prep:pro')\n(u'qani\\u0304m', 'n')\n('be', 'prep')\n(u'cura\\u0304', 'n')\n(u'\\u0294axe\\u0304ret', 'adj')\n(u'nak\\u0323o\\u0304n_me\\u0294o\\u0304d', 'co')\n('ze', 'pro:dem')\n(u'bama\\u0304', 'n')\n(u's\\u030cel', 'prep')\n(u'te\\u0294at\\u0323ro\\u0304n', 'n')\n('we', 'conj')\n(u'yos\\u030cvi\\u0304m', 'part')\n('we', 'conj')\n(u'ro\\u0294i\\u0304m', 'part')\n(u'\\u0294e\\u0304yze', 'que')\n(u'hacaga\\u0304', 'n')\n(u'zo\\u0294t', 'pro:dem')\n(u'\\u0294ahha', 'voc')\n(u'\\u0294e\\u0304yfo', 'que')\n(u's\\u030cot\\u0323e\\u0304r', 'n')\n(u'\\u0294ahha', 'voc')\n(u'nak\\u0323o\\u0304n', 'co')\n('ze', 'pro:dem')\n(u'xaya\\u0304l', 'n')\n('bdil', 'n')\n('ma', 'que')\n('ze', 'pro:dem')\n(u'kaze\\u0304', 'pro:dem')\n('we', 'conj')\n(u'kaze\\u0304', 'pro:dem')\n('we', 'conj')\n(u'kaze\\u0304', 'pro:dem')\n(u'Li\\u0294o\\u0304ri', 'n:prop')\n(u'Li\\u0294o\\u0304rile\\u0304', 'n:prop')\n(u'hi\\u0304ne', 'co')\n(u'yalda\\u0304', 'n')\n(u'Li\\u0294o\\u0304riqu', 'n:prop')\n(u'Li\\u0294o\\u0304r', 'n:prop')\n(u'nak\\u0323o\\u0304n', 'adv')\n('ken', 'adv')\n('ken', 'adv')\n('ha', 'det')\n(u'\\u0294eme\\u0304t', 'n')\n(u'hi\\u0294', 'cop')\n(u's\\u030ce', 'conj:subor')\n(u'be\\u0304t\\u0323ax', 'adv')\n('ha', 'det')\n('xala', 'n')\n('ha', 'det')\n(u'sipu\\u0304r', 'n')\n(u'\\u0295al', 'prep')\n('ha', 'det')\n(u'xalali\\u0304t', 'n')\n('kvar', 'adv')\n(u'nigma\\u0304r', 'v')\n(u'\\u0294ava\\u0304l', 'conj')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'lo\\u0294', 'neg')\n(u's\\u0323a\\u0304mti_lev', 'v')\n(u'\\u0294o\\u0304rili', 'n:prop')\n('way', 'co')\n('way', 'co')\n('way', 'co')\n('way', 'co')\n(u'\\u0294e\\u0304yze', 'que')\n(u'paxa\\u0304d', 'v')\n(u'\\u0294at', None)\n(u'yoda\\u0304\\u0295at', None)\n('ha', 'det')\n(u'tola\\u0304\\u0295at', 'n')\n(u'lo\\u0294', 'neg')\n(u'mafxida\\u0304', 'part')\n(u'tola\\u0304\\u0295at', 'n')\n(u'qt\\u0323ana\\u0304', 'part')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'hi\\u0294', 'pro:person')\n(u'mistake\\u0304let', 'part')\n(u'\\u0295al', 'prep')\n('ha', 'det')\n(u'tani\\u0304n', 'n')\n('we', 'conj')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'xos\\u030ce\\u0304vet', 'part')\n(u's\\u030ce', 'conj:subor')\n('ha', 'det')\n(u'qat\\u0323a\\u0304n\\xe7uq', 'a:dim')\n('ha_ze', 'pro:dem')\n(u'qor\\u0294i\\u0304m', 'part')\n('lo', 'prep:pro')\n(u'Go\\u0304nzo', 'n:prop')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'xos\\u030ce\\u0304vet', 'part')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'lo\\u0294', 'neg')\n(u'bet\\u0323uxa\\u0304', 'adj')\n(u'lo\\u0294', 'co')\n('ze', 'pro:dem')\n(u'tani\\u0304n', 'n')\n(u'kmo_s\\u030ce', 'conj:subor')\n(u'\\u0294ama\\u0304rt', 'v')\n('ze', 'pro:dem')\n(u'tola\\u0304\\u0295at', 'n')\n(u'kmo_s\\u030ce', 'conj:subor')\n(u'\\u0294ama\\u0304rt', 'v')\n(u'tani\\u0304n', 'n')\n(u'Go\\u0304nzo', 'n:prop')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'lo\\u0294', 'neg')\n(u'yada\\u0304\\u0295ti', 'v')\n(u'\\u0294eyk\\u0323', 'que')\n(u'qor\\u0294i\\u0304m', 'part')\n('lo', 'prep:pro')\n(u'\\u0294ava\\u0304l', 'conj')\n('ze', 'pro:dem')\n(u'qarna\\u0304f', 'n')\n(u'\\u0294ahha', 'voc')\n('ken', 'adv')\n('ken', 'co')\n('gam', 'qn')\n(u'qarna\\u0304f', 'n')\n('ze', 'pro:dem')\n(u'lo\\u0294', 'neg')\n(u'xaya\\u0304', 'n')\n(u'ne\\u0295ima\\u0304', 'adj')\n(u'bimyuxa\\u0304d', 'adv')\n(u'\\u0294ahha', 'voc')\n(u'nak\\u0323o\\u0304n', 'co')\n(u'yak\\u0323o\\u0304l', 'part')\n(u'lihiyo\\u0304t', 'cop')\n(u'met\\u0323ape\\u0304let', 'part')\n(u'met\\u0323ape\\u0304let', 'part')\n(u'met\\u0323ape\\u0304let', 'part')\n(u'met\\u0323ape\\u0304let', 'part')\n(u'met\\u0323ape\\u0304let', 'part')\n(u'met\\u0323ape\\u0304let', 'part')\n(u'bewada\\u0304\\u0294y_s\\u030ce', 'conj:subor')\n(u'hi\\u0294', 'pro:person')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'hi\\u0294', 'pro:person')\n(u'lo\\u0294', 'neg')\n(u'tino\\u0304qet', 'n')\n(u'hi\\u0294', 'pro:person')\n(u'\\u0294is\\u030ca\\u0304', 'n')\n(u'Li\\u0294o\\u0304ri', 'n:prop')\n(u'\\u0294ahha', 'voc')\n(u'nak\\u0323o\\u0304n', 'adv')\n(u'\\u0294ahha', 'voc')\n(u'nak\\u0323o\\u0304n', 'adv')\n(u'nak\\u0323o\\u0304n', 'adv')\n(u'nak\\u0323o\\u0304n', 'co')\n(u'\\u0294az', 'adv')\n('ma', 'que')\n('ze', 'pro:dem')\n(u'sima\\u0304n', 'n')\n(u'\\u0294im', 'conj:subor')\n(u'hu\\u0294', 'pro:person')\n(u'yak\\u0323o\\u0304l', 'part')\n(u'linbo\\u0304ax', 'v')\n('we', 'conj')\n('gam', 'adv')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'hu\\u0294', 'pro:person')\n(u'ke\\u0304lev', 'n')\n(u'gado\\u0304l', 'adj')\n(u'yaxasi\\u0304t', 'adv')\n('we', 'conj')\n(u'kaze\\u0304', 'pro:dem')\n('we', 'conj')\n(u'kaze\\u0304', 'pro:dem')\n('ze', 'pro:dem')\n('', 'adv')\n(u'yafe\\u0304', 'adv')\n('ze', 'pro:dem')\n(u'Qe\\u0304rmit\\u0323_ha_cfarde\\u0304a\\u0295', 'n:prop')\n(u'Nica\\u0304n', 'n:prop')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'tipo\\u0304l', 'v')\n(u'\\u0294ahha', 'voc')\n('we', 'conj')\n(u'ze\\u0304hu', 'co')\n(u'\\u0295ak\\u0323s\\u030ca\\u0304yw', 'adv')\n(u'\\u0295ak\\u0323s\\u030ca\\u0304yw', 'adv')\n(u'\\u0295ak\\u0323s\\u030ca\\u0304yw', 'adv')\n(u'\\u0295ak\\u0323s\\u030ca\\u0304yw', 'adv')\n(u'\\u0295ak\\u0323s\\u030ca\\u0304yw', 'adv')\n(u'\\u0294eyn', 'exs')\n(u'lak\\u0323', 'prep:pro')\n(u'ko\\u0304ax', 'n')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n(u'\\u0294at', 'pro:person')\n('ma', 'que')\n('ma', 'que')\n(u'lehistake\\u0304l', 'v')\n('ma', 'que')\n('ze', 'pro:dem')\n('ma', 'que')\n('ze', 'pro:dem')\n(u'nidba\\u0304q', 'v')\n(u'lak\\u0323', 'prep:pro')\n(u'qo\\u0304rnfleqs', 'n')\n(u'\\u0294exa\\u0304d', 'num')\n('ma', 'que')\n(u'mitxas\\u030ce\\u0304q', 'part')\n(u'lak\\u0323', 'prep:pro')\n(u'la\\u0295as\\u0323o\\u0304t', 'v')\n(u'\\u0294ahha', 'voc')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'\\u0294ula\\u0304y', 'adv')\n(u'\\u0294axa\\u0304r_kak\\u0323', 'adv')\n(u'tes\\u030cvi\\u0304', 'v')\n('we', 'conj')\n(u'to\\u0294k\\u0323li\\u0304', 'v')\n(u's\\u030cam', 'adv')\n(u'bik\\u0323la\\u0304l', 'adv')\n(u't\\u0323ov', 'co')\n(u'\\u0294az', 'adv')\n(u'to\\u0294k\\u0323li\\u0304', 'v')\n(u'bete\\u0294avo\\u0304n', 'adv')\n(u'\\u0294ula\\u0304y', 'adv')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'\\u0294avo\\u0304\\u0294', 'v')\n(u'las\\u030ce\\u0304vet', 'v')\n(u'leyade\\u0304k\\u0323', 'prep:pro')\n('ma', 'que')\n(u't\\u0323ov', 'co')\n('ma', 'que')\n('ze', 'pro:dem')\n('ma', 'que')\n('ze', 'pro:dem')\n('mi', 'prep')\n('qoci', 'chi')\n(u'mevina\\u0304', 'part')\n(u'mevina\\u0304', 'part')\n(u'mevina\\u0304', 'part')\n(u'mevina\\u0304', 'part')\n(u'mevina\\u0304', 'part')\n(u'mevina\\u0304', 'part')\n('ze', 'pro:dem')\n('kmo', 'prep')\n(u'tino\\u0304q', 'n')\n(u'diba\\u0304rt', 'v')\n('kmo', 'prep')\n(u'tino\\u0304q', 'n')\n(u'\\u0294oy', 'co')\n('ha', 'det')\n(u'ricpa\\u0304', 'n')\n(u'mele\\u0294a\\u0304', 'adj')\n(u'qo\\u0304renfleqs', 'n')\n(u'\\u0294i_\\u0294efs\\u030ca\\u0304r', 'adv')\n(u'lale\\u0304k\\u0323et', 'v')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294az', 'adv')\n(u'\\u0294at', 'pro:person')\n(u'xak\\u0323ama\\u0304', 'adj')\n('ba', 'prep')\n(u'bo\\u0304qer', 'n')\n(u'zore\\u0304qet', 'part')\n(u'qo\\u0304renfleqs', 'n')\n(u'\\u0295al', 'prep')\n('ha', 'det')\n(u'ricpa\\u0304', 'n')\n(u'nak\\u0323o\\u0304n', 'co')\n(u'xak\\u0323u\\u0304me', 'neo')\n(u'xak\\u0323u\\u0304me', 'neo')\n('ma', 'que')\n('ma', 'que')\n(u'la\\u0295as\\u0323o\\u0304t', 'v')\n(u'lak\\u0323', 'prep:pro')\n(u'\\u0295od', 'qn')\n(u'\\u0294exa\\u0304d', 'num')\n('ma', 'que')\n(u'Li\\u0294o\\u0304r', 'n:prop')\n('ma', 'que')\n(u'meluk\\u0323la\\u0304k\\u0323', 'adj')\n('ha', 'det')\n(u'ricpa\\u0304', 'n')\n(u'niqi\\u0304ti', 'v')\n(u'\\u0295ak\\u0323s\\u030ca\\u0304yw', 'adv')\n(u'lo\\u0294', 'neg')\n(u'ra\\u0294i\\u0304t', 'v')\n(u's\\u030ce', 'conj:subor')\n(u't\\u0323i\\u0294t\\u0323e\\u0304\\u0294ti', 'v')\n('nu', 'co')\n(u'ze\\u0304hu', 'co')\n(u'\\u0294az', 'adv')\n(u'\\u0295ak\\u0323s\\u030ca\\u0304yw', 'adv')\n('ze', 'pro:dem')\n('kvar', 'adv')\n(u'lo\\u0294', 'neg')\n(u'meluk\\u0323la\\u0304k\\u0323', 'adj')\n(u'tavi\\u0304\\u0294', 'v')\n(u'Nica\\u0304n', 'n:prop')\n(u'bo\\u0294', 'v')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'\\u0294es\\u0323axe\\u0304q', 'v')\n(u'\\u0294itk\\u0323a\\u0304', 'prep:pro')\n(u'tir\\u0294e\\u0304', 'v')\n(u'\\u0294eyk\\u0323', 'que')\n(u'mes\\u0323axqi\\u0304m', 'part')\n(u'tir\\u0294e\\u0304', 'v')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u's\\u030ce', 'conj:subor')\n(u'tigmeri\\u0304', 'v')\n('be', 'prep')\n(u'pa\\u0304\\u0295am_\\u0294axa\\u0304t', 'adv')\n(u't\\u0323ov', 'co')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'heva\\u0304nt', 'v')\n(u'\\u0294et', 'acc')\n('ze', 'pro:dem')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u'\\u0294et', 'acc')\n(u't\\u0323ov', 'co')\n(u're\\u0304ga\\u0295', 'n')\n(u'Nica\\u0304n', 'n:prop')\n('qum', 'v')\n(u'\\u0294ula\\u0304y', 'adv')\n(u'\\u0294at', 'pro:person')\n(u'roca\\u0304', 'part')\n('gam', 'adv')\n(u'ma\\u0304s\\u030cehu', 'pro:indef')\n(u'mezi\\u0304n', 'adj')\n(u'Li\\u0294o\\u0304r', 'n:prop')\n('gam', 'qn')\n(u'qo\\u0304renfleqs', 'n')\n('ze', 'pro:dem')\n(u'mezi\\u0304n', 'part')\n(u'\\u0294im_ki', 'adv')\n(u'lo\\u0294_mi_yode\\u0304a\\u0295_ma', 'adv')\n(u'yes\\u030c', 'exs')\n('le', 'prep')\n('ze', 'pro:dem')\n(u're\\u0304yax', 'n')\n(u's\\u030cel', 'prep')\n(u'Ba\\u0304mba', 'n:prop')\n(u'civ\\u0295oni\\u0304t', 'adj')\n('ze', 'pro:dem')\n('raq', 'qn')\n(u're\\u0304yax', 'n')\n(u's\\u030cel', 'prep')\n(u'Ba\\u0304mba', 'n:prop')\n(u'civ\\u0295oni\\u0304t', 'adj')\n(u'\\u0294at', 'pro:person')\n(u'mitkawe\\u0304net', 'part')\n(u'hi\\u0304ne', 'co')\n(u'yo\\u0304yo', 'n')\n(u'yo\\u0304yo', 'n')\n(u'cari\\u0304k\\u0323', 'adv')\n(u'cari\\u0304k\\u0323', 'adv')\n(u'cari\\u0304k\\u0323', 'adv')\n(u'cari\\u0304k\\u0323', 'adv')\n(u're\\u0304ga\\u0295', 'co')\n(u'na\\u0304na', 'n')\n(u'\\u0294ahha', 'voc')\n(u'qu\\u0304qu', 'co')\n('vadam', 'chi')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'mexaqa\\u0304', 'part')\n(u'\\u0294et', 'acc')\n(u'Nica\\u0304n', 'n:prop')\n('ha', 'det')\n(u'\\u0294eme\\u0304t', 'n')\n(u'hi\\u0294', 'cop')\n(u's\\u030ce', 'conj:subor')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'lo\\u0294', 'neg')\n(u'yoda\\u0304\\u0295at', 'part')\n('ma', 'que')\n('ze', 'pro:dem')\n('ba', 'prep')\n(u's\\u0323afa\\u0304', 'n')\n(u's\\u030celo\\u0304', 'prep:pro')\n('ma', 'que')\n('ze', 'pro:dem')\n('pata', 'chi')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'yes\\u030c', 'exs')\n('lo', 'prep:pro')\n(u'kol_mine\\u0304y', 'qn')\n(u'mili\\u0304m', 'n')\n(u's\\u030ce', 'conj:subor')\n(u'\\u0294ani\\u0304', 'pro:person')\n(u'lo\\u0294', 'neg')\n(u'mevina\\u0304', 'part')\n(u'\\u0294a\\u0304dam', 'n:prop')\n(u'hi\\u0304ne', 'co')\n(u'hu\\u0294', 'pro:person')\n(u'\\u0294ome\\u0304r', 'part')\n(u'\\u0294a\\u0304dam', 'n:prop')\n(u'hu\\u0294', 'pro:person')\n(u'\\u0294ome\\u0304r', 'part')\n(u'\\u0294a\\u0304dam', 'n:prop')\n(u'\\u0294a\\u0304dam', 'n:prop')\n(u'\\u0294ata\\u0304', 'pro:person')\n(u'yode\\u0304a\\u0295', 'part')\n(u'lehagi\\u0304d', 'v')\n('ma', 'que')\n('ze', 'pro:dem')\n('ha', 'det')\n(u'mila\\u0304', 'n')\n(u's\\u030cel', 'prep')\n(u'\\u0294a\\u0304dam', 'n:prop')\n('ma', 'que')\n(u'peru\\u0304s\\u030c', 'n')\n('ha', 'det')\n(u'mila\\u0304', 'n')\n(u'\\u0294a\\u0304dam', 'n:prop')\n(u'\\u0294a\\u0304dam', 'n:prop')\n(u'hu\\u0294', 'pro:person')\n(u'be\\u0295e\\u0304cem', 'adv')\n(u'kmo_s\\u030ce', 'conj:subor')\n(u'\\u0294ana\\u0304xnu', 'pro:person')\n(u'nohagi\\u0304m', 'part')\n(u'lehagi\\u0304d', 'v')\n('be', 'prep')\n(u'\\u0295ivri\\u0304t', 'n:prop')\n(u'\\u0294ada\\u0304m', 'n:prop')\n(u'haya\\u0304', 'cop')\n('ha', 'det')\n(u'\\u0294is\\u030c', 'n')\n('ha', 'det')\n(u'ri\\u0294s\\u030co\\u0304n', 'adj')\n(u'lefi\\u0304', 'prep')\n('ha', 'det')\n(u'tora\\u0304', 'n')\n('ha', 'det')\n(u'yehudi\\u0304t', 'adj')\n(u's\\u030ce', 'conj:subor')\n(u'nivra\\u0304\\u0294', 'part')\n(u'\\u0295ale\\u0304y_\\u0294adamo\\u0304t', 'adv')\n(u'\\u0294elohi\\u0304m', 'n')\n(u'bara\\u0304\\u0294', 'v')\n(u'\\u0294ada\\u0304m', 'n')\n(u'nak\\u0323o\\u0304n', 'adv')\n(u'\\u0295al', None)\n('ma', None)\n(u'himre\\u0304\\u0294nu', None)\n(u'me\\u0295a\\u0304l', 'prep')\n('la', 'prep')\n(u'\\u0295anani\\u0304m', 'n')\n(u'nak\\u0323o\\u0304n', 'co')\n(u'lo\\u0294', 'neg')\n(u'\\u0295aft', 'v')\n(u'\\u0294o', 'conj')\n(u'lo\\u0294', 'neg')\n(u'\\u0294aha\\u0304vt', 'v')\n(u'be\\u0304t\\u0323ax', 'adv')\n(u's\\u030ce', 'conj:subor')\n(u'lo\\u0294', 'neg')\n(u'\\u0295aft', 'v')\n(u'hayi\\u0304t', 'cop')\n(u'beto\\u0304k\\u0323', 'prep')\n('ha', 'det')\n(u'mat\\u0323o\\u0304s', 'n')\n(u'lo\\u0294', 'neg')\n(u'paxa\\u0304det', 'v')\n(u'yalda\\u0304', 'n')\n(u'\\u0294amica\\u0304', 'adj')\n('ha', 'det')\n('pax', 'n')\n('ba', 'prep')\n(u'ricpa\\u0304', 'n')\n(u'mas\\u030cmi\\u0304a\\u0295', 'part')\n('qol', 'n')\n(u'muza\\u0304r', 'adj')\n('ding_dong', 'on')\n('ding_dong', 'on')\n(u'lo\\u0294', 'neg')\n(u'yoda\\u0304\\u0295at', 'part')\n(u'hole\\u0304k\\u0323et', 'part')\n(u'hole\\u0304k\\u0323et', 'part')\n(u'hole\\u0304k\\u0323et', 'part')\n(u'hole\\u0304k\\u0323et', 'part')\n(u'hole\\u0304k\\u0323et', 'part')\n(u'hole\\u0304k\\u0323et', 'part')\n('ma', 'que')\n(u'lo\\u0294', 'neg')\n(u'hi\\u0304ne', 'co')\n(u'yes\\u030c', 'exs')\n(u'lak\\u0323', 'prep:pro')\n('kol', 'qn')\n('ha', 'det')\n(u'qufsa\\u0304', 'n')\n(u'leyade\\u0304k\\u0323', 'prep:pro')\n(u'bete\\u0294avo\\u0304n', 'co')\n(u'\\u0295al_lo\\u0294_dava\\u0304r', 'co')\n"
                        }
                    ]
                }
            },
            "evaluatorReader": true,
            "lineCount": 86
        },
        {
            "id": "codeLf0EKJ",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "def lines_from_single_file(file_name):",
                    "    # Inputs a file_name",
                    "    # Outputs a list of words in that file",
                    "    with open(file_name, \"r\") as fin:",
                    "        contents = fin.read().decode('utf8').splitlines()",
                    "    return contents",
                    "    ",
                    "def read_words_from_files(child_list, data_dir):",
                    "    # Inputs a list of child names and a directory to read from",
                    "    # Outputs dictionary with keys child names and values a dictionary ",
                    "    # with keys file path and values a list of words corresponding to that file",
                    "    # {child_name : {file_name : [word1, word2,...]} ...}",
                    "    corpus_words = {}",
                    "    stat_types = [\"child\", \"mother\", \"child_stem\", \"mother_stem\"]",
                    "    for child in child_list:",
                    "        corpus_words[child] = {}",
                    "        for stat_type in stat_types:",
                    "            path = data_dir + child + \"/\" + stat_type + \"/\"",
                    "            files = os.listdir(path)",
                    "            for f in files:",
                    "                single_words = lines_from_single_file(path + f)",
                    "                (corpus_words[child])[path + f] = single_words",
                    "    return corpus_words",
                    "",
                    "def lines_to_sentence_list(file_name):",
                    "    # Inputs a file name",
                    "    # Outputs a list of sentences (a list of words) corresponding to that file",
                    "    with open(file_name, \"r\") as fin:",
                    "        contents = fin.read().decode('utf8').splitlines()",
                    "    sentence_list = []",
                    "    for sentence in contents:",
                    "        x = (sentence.split(' '))",
                    "        sentence_list.append(x)",
                    "#     print sentence_list # ASK MOSCOSO ABOUT WORD CONTRACTIONS, UNDERSCORES",
                    "    return sentence_list",
                    "",
                    "def read_sentences_from_files(child_list, data_dir):",
                    "    # Inputs a list of child names and a data directory",
                    "    # Outputs dictionary with keys child names and value dictionary ",
                    "    # with keys file path and values list of sentences corresponding to that file",
                    "    sentences = {}",
                    "    stat_types = [\"child_sentences\", \"mother_sentences\"]",
                    "    for child in child_list:",
                    "        sentences[child] = {}",
                    "        for stat_type in stat_types:",
                    "            path = data_dir + child + \"/\" + stat_type + \"/\"",
                    "            files = os.listdir(path)",
                    "            for f in files:",
                    "                single_sentences = lines_to_sentence_list(path + f)",
                    "                (sentences[child])[path + f] = single_sentences",
                    "    return sentences",
                    "",
                    "# 10 seconds to run",
                    "child_list = child_files_dict.keys()",
                    "corpus_words = read_words_from_files(child_list, data_directory)",
                    "sentences = read_sentences_from_files(child_list, data_directory)"
                ]
            },
            "output": {
                "state": {},
                "selectedType": "BeakerDisplay",
                "pluginName": "IPython",
                "shellId": "2AFA94D3B37A45E9B56AF14B18DB0EB6",
                "elapsedTime": 1114
            },
            "evaluatorReader": true,
            "lineCount": 56
        },
        {
            "id": "code2CHrOj",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "def get_mean_length_utterances(window_files, child):",
                    "    # Syntactic Diversity",
                    "    # Inputs a window of files and a child name",
                    "    # Outputs the MLU of the child and mother in that window of files",
                    "    child_sents = []",
                    "    mother_sents = []",
                    "    for window_file in window_files:",
                    "        child_sents += sentences[child][data_directory + child + \"/\" + \"child_sentences\" + \"/\" + window_file]",
                    "        mother_sents += sentences[child][data_directory + child + \"/\" + \"mother_sentences\" + \"/\" + window_file]",
                    "    child_len_words = [] ",
                    "    for i in range(len(child_sents)):",
                    "        child_len_words.append(len(child_sents[i]))",
                    "    child_MLU = np.mean(child_len_words)",
                    "    mother_len_words = [] ",
                    "    for i in range(len(mother_sents)):",
                    "        mother_len_words.append(len(mother_sents[i]))",
                    "    mother_MLU = np.mean(mother_len_words)",
                    "    return child_MLU, mother_MLU",
                    "",
                    "def make_freq_dist_files(window_files, child, use_stem):",
                    "    # Inputs a window of files, a child name, and a stem flag",
                    "    # Outputs a Freq dist for the both the child and mother (stemmed if specified)",
                    "    child_words = []",
                    "    mother_words = []",
                    "    if(use_stem):",
                    "        for window_file in window_files:",
                    "            # Remove age from words",
                    "            child_words += (corpus_words[child][data_directory + child + '/child_stem/' + window_file])[1:]",
                    "            mother_words += (corpus_words[child][data_directory + child + '/mother_stem/' + window_file])[1:]",
                    "    else:",
                    "        for window_file in window_files:",
                    "            # Remove age from words",
                    "            child_words += (corpus_words[child][data_directory + child + '/child/' + window_file])[1:]",
                    "            mother_words += (corpus_words[child][data_directory + child + '/mother/' + window_file])[1:]",
                    "    return FreqDist(child_words), FreqDist(mother_words)",
                    "",
                    "def correct(list_files):",
                    "    # Inputs a list of files",
                    "    # Outputs \"anne01a\" type file names",
                    "    correct_file_names = []",
                    "    for i in range(len(list_files)):",
                    "        correct_file_names.append((((list_files[i])[11:-4]).split(\"/\"))[1])",
                    "    return correct_file_names",
                    "",
                    "def make_windows(window_size, child):",
                    "    # Inputs a window size of child and a child name",
                    "    # Outputs a list of window_size window_files corresponding to that child",
                    "    list_window_files = []",
                    "    if len(child_files_dict[child]) < window_size:",
                    "        # Windows cannot be larger than the number of files per child",
                    "        raise ValueError(\"Requested larger window than number of files.\") ",
                    "    else:",
                    "        i = 0",
                    "        while(i + window_size <= len(child_files_dict[child])):",
                    "            correct_file_names = correct(child_files_dict[child][i:i + window_size])",
                    "            list_window_files.append(correct_file_names)",
                    "            i += 1",
                    "    return list_window_files",
                    "",
                    "def window_to_weighted_age(window_files, child):",
                    "    # Inputs a single window file of child and a child name",
                    "    # Outputs its weighted (by number of words) age in that window",
                    "    weighted_age = 0.0",
                    "    number_of_words = []",
                    "    for window_file in window_files:",
                    "        data = corpus_words[child][data_directory + child + '/child/' + window_file]",
                    "        n = sum(array((FreqDist(data)).values()))",
                    "        number_of_words.append(n)",
                    "        window_age = int(data[0]) * n",
                    "        weighted_age += window_age",
                    "    weighted_age /= sum(number_of_words) ",
                    "    return weighted_age"
                ]
            },
            "output": {
                "state": {},
                "selectedType": "BeakerDisplay",
                "pluginName": "IPython",
                "shellId": "2AFA94D3B37A45E9B56AF14B18DB0EB6",
                "elapsedTime": 360
            },
            "evaluatorReader": true,
            "lineCount": 72
        },
        {
            "id": "codeBiZG51",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "import time",
                    "",
                    "start = time.time()",
                    "window_size = 3",
                    "fout = open(nmmfile,\"w\")",
                    "print >> fout, \"Child Age N.child H.child.S H.child.I Schild N.mother H.mother.S H.mother.I Smother\"",
                    "for child in child_list:",
                    "    for window in make_windows(window_size, child):",
                    "        # Age",
                    "        age = window_to_weighted_age(window, child)",
                    "        # Freq Dists",
                    "        fchild, fmother = make_freq_dist_files(window, child, False)",
                    "        fchildS, fmotherS = make_freq_dist_files(window, child, True)",
                    "        # Statistics",
                    "        nchild = sum(array(fchild.values()))",
                    "        nmother = sum(array(fmother.values()))",
                    "        # Entropies",
                    "        Hchild = Ent.Entropy(fchild,method=\"CWJ\")",
                    "        Hmother = Ent.Entropy(fmother,method=\"CWJ\")",
                    "        # Entropies (stemmed - Lexical Diversity)",
                    "        HchildS = Ent.Entropy(fchildS,method=\"CWJ\")",
                    "        HmotherS = Ent.Entropy(fmotherS,method=\"CWJ\")",
                    "        # Inflectional Diversity",
                    "        HchildI = Hchild - HchildS",
                    "        HmotherI = Hmother - HmotherS",
                    "        # Syntactic Diversity (MLU) BOTTLENECK",
                    "        Schild, Smother = get_mean_length_utterances(window, child)",
                    "        ",
                    "#         print child,age,nchild,HchildS,HchildI,Schild,nmother,HmotherS,HmotherI,Smother",
                    "        print >> fout, child,age,nchild,HchildS,HchildI,Schild,nmother,HmotherS,HmotherI,Smother",
                    "",
                    "fout.close()",
                    "print \"Runtime for window_size \" + str(window_size) + \" is \" + str(time.time() - start)",
                    "# 18s to run for w_size = 3"
                ]
            },
            "output": {
                "state": {},
                "result": {
                    "type": "Results",
                    "outputdata": [
                        {
                            "type": "err",
                            "value": "/usr/local/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
                        },
                        {
                            "type": "out",
                            "value": "Runtime for window_size 3 is 3.64753890038\n"
                        }
                    ]
                },
                "selectedType": "Results",
                "pluginName": "IPython",
                "shellId": "2AFA94D3B37A45E9B56AF14B18DB0EB6",
                "elapsedTime": 3821
            },
            "evaluatorReader": true,
            "lineCount": 34
        }
    ],
    "namespace": {}
}
