{
    "beaker": "2",
    "evaluators": [
        {
            "name": "Html",
            "plugin": "Html",
            "view": {
                "cm": {
                    "mode": "htmlmixed"
                }
            }
        },
        {
            "name": "JavaScript",
            "plugin": "JavaScript",
            "jsSetting2": "",
            "jsSetting1": "",
            "view": {
                "cm": {
                    "mode": "javascript",
                    "background": "#FFE0F0"
                }
            }
        },
        {
            "name": "IPython",
            "plugin": "IPython",
            "setup": "%matplotlib inline\nimport numpy\nimport matplotlib\nfrom matplotlib import pylab, mlab, pyplot\nnp = numpy\nplt = pyplot\nfrom IPython.display import display\nfrom IPython.core.pylabtools import figsize, getfigs\nfrom pylab import *\nfrom numpy import *\n",
            "view": {
                "cm": {
                    "mode": "python"
                }
            }
        }
    ],
    "cells": [
        {
            "id": "code7ZMmv7",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "from nltk import data",
                    "from nltk.corpus.reader import CHILDESCorpusReader",
                    "from nltk.probability import FreqDist",
                    "import entropies7.entropies7 as Ent",
                    "from numpy import *",
                    "import scipy as sp",
                    "import numpy as np",
                    "import re",
                    "import os as os",
                    "",
                    "%matplotlib inline",
                    "from pandas import *",
                    "import matplotlib.pyplot as plt",
                    "from matplotlib.font_manager import FontProperties",
                    "",
                    "def convert_age_to_days(age):",
                    "    # Takes P[0-9]Y[1-12]M[1-31]D",
                    "    # Outputs integer number of days",
                    "    parsed_age = re.split('P|Y|M|D',age)",
                    "    years = parsed_age[1]",
                    "    months = parsed_age[2]",
                    "    days = parsed_age[3]",
                    "    if days == '':",
                    "        days = 0",
                    "    return int(years) * 365 + int(months) * 30 + int(days)",
                    ""
                ]
            },
            "output": {
                "state": {},
                "selectedType": "BeakerDisplay",
                "pluginName": "IPython",
                "shellId": "E45A8F63109D4A4797D9041015895E72",
                "elapsedTime": 330
            },
            "evaluatorReader": true,
            "lineCount": 26
        },
        {
            "id": "codeVkbQ0l",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "def get_files(corpus):",
                    "    # Inputs the name of the corpus ",
                    "    # Outputs:",
                    "    # corpus_root: string local path to corpora /.../nltk_data/corpora/childes/[corpus]",
                    "    # heb: CHILDESCorpusReader tool",
                    "    # files: lsit of name of files in corpus_root",
                    "    # data_directory: string location of future output data",
                    "    corpus_root = data.find('corpora/childes/Hebrew-MOR')",
                    "    heb = CHILDESCorpusReader(corpus_root, 'BermanLong/.*xml')",
                    "#     #CHANGE THIS",
                    "#     if(corpus_root == \"/Users/dspoka/nltk_data/corpora/childes/English-UK-MOR\"):",
                    "#         data_directory = \"/Users/dspoka/Desktop/moscoso/nltk_childes/NLTKCHILDES/\"",
                    "#     else:",
                    "    data_directory = \"/Users/jeremyirvin/Desktop/SeniorThesis/Childes/nltk_childes/Hebrew/data_directory/\"",
                    "    files = heb.fileids()",
                    "    return corpus_root, heb, files, data_directory",
                    "# %timeit files = get_files('childes')",
                    "corpus_root, heb, files, data_directory = get_files('manchester')",
                    "nmmfile = data_directory + \"morph-heb.csv\"",
                    "nmsfile = data_directory + \"syntax-heb.csv\"",
                    "# childes has 804 files"
                ]
            },
            "output": {
                "state": {},
                "selectedType": "BeakerDisplay",
                "pluginName": "IPython",
                "shellId": "E45A8F63109D4A4797D9041015895E72",
                "elapsedTime": 307
            },
            "evaluatorReader": true,
            "lineCount": 21
        },
        {
            "id": "code4H0LWX",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "def compare(f1, f2):",
                    "    # Inputs two files",
                    "    # Outputs -1 if the child is younger in f1, 1 if child older in f1, 0 if same age",
                    "    age1 = convert_age_to_days(heb.age(f1)[0])",
                    "    age2 = convert_age_to_days(heb.age(f2)[0])",
                    "    if age1 < age2:",
                    "        return -1",
                    "    elif age2 < age1:",
                    "        return 1",
                    "    else:",
                    "        return 0",
                    "   ",
                    "def group_files_by_child_age(files):",
                    "    # Inputs list of all filenames",
                    "    # Outputs dictionary with keys child names and values list of file names (sorted by age) corresponding to child",
                    "    child_files_dict = {}",
                    "    for f in files:",
                    "        child = f.split(\"/\")[1]",
                    "        if child_files_dict.has_key(child):",
                    "            child_files_dict[child] += [f]",
                    "        else:",
                    "            child_files_dict[child] = [f]",
                    "    for child in child_files_dict.keys():",
                    "        child_files_dict[child] = sorted(child_files_dict[child], cmp=compare)",
                    "    return child_files_dict",
                    "",
                    "# %timeit child_files_dict = group_files_by_child_age(files)",
                    "# 30s to run",
                    "child_files_dict = group_files_by_child_age(files)"
                ]
            },
            "output": {
                "state": {},
                "selectedType": "BeakerDisplay",
                "pluginName": "IPython",
                "shellId": "E45A8F63109D4A4797D9041015895E72",
                "elapsedTime": 36906
            },
            "evaluatorReader": true,
            "lineCount": 29
        },
        {
            "id": "codezCDEdg",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "# \"CHI, MOT, FAT, RAY, GMO, GFA, GFO, GRM, GRF, INV, NIC, SEF, TAL, VIS, YOX, YAE\"",
                    "f = 'BermanLong/hagar/hag107a.xml'",
                    "child_words = heb.words(f,speaker=\"CHI\",replace=True)",
                    "child_stems = [w.split(\"-\")[0] for w in heb.words(f,speaker=\"CHI\",replace=True,stem=True)]",
                    "#NOTE: NOT ALL FILES HAVE MOTHER AS A SPEAKER - WILL YIELD 0'S AND NAN'S"
                ]
            },
            "output": {
                "state": {},
                "result": {
                    "type": "Results",
                    "outputdata": [
                        {
                            "type": "out",
                            "value": "yādi\nyādi\nyādi\nyādi\nrocā\nlesapēr\nlesapēr\nnadnēd\nhigāʕti\nnadnēd\nnadnēd\nnaʕalāyim\nnaʕalāyim\nlašīr\nlašīr\nlašīr\nlašīr\nlašīr\nlašīr\nʕeynāyim\nʕeynāyim\nmitgalēax\nmitgalēax\nmitgalēax\nmitgalēax\nmitgalēax\nmiḳnasāyim\nmiḳnasāyim\n\n\nyād\nyād\nyād\nyād\nracā\nsipēr\nsipēr\nnidnēd\nhigīaʕ\nnidnēd\nnidnēd\nnāʕal\nnāʕal\nʕāyin\nʕāyin\nhitgalēax\nhitgalēax\nhitgalēax\nhitgalēax\nhitgalēax\nmiḳnās\nmiḳnās\n"
                        }
                    ]
                },
                "selectedType": "Results",
                "pluginName": "IPython",
                "shellId": "E45A8F63109D4A4797D9041015895E72",
                "elapsedTime": 312,
                "dataresult": "577"
            },
            "evaluatorReader": true,
            "lineCount": 5
        },
        {
            "id": "codeVEEjSo",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "def write_list_to_file(f, file_path, use_stem, speaker):",
                    "    # Inputs a single child's file name, an out file_path name, a use_stem flag, and speaker (MOT or CHI)",
                    "    # Writes (to the file file_path) the age of the child in the file f and then the list of all words (or stems) used in f, newline separated",
                    "    fout = open(file_path, \"w\")",
                    "    if(use_stem == False):",
                    "        words = heb.words(f,speaker=speaker,replace=True)",
                    "        fout.write(str(convert_age_to_days(heb.age(f)[0])) + \"\\n\")",
                    "        for word in words:",
                    "            if(word != \"xxx\"):",
                    "                fout.write(word.encode('utf8') + \"\\n\")",
                    "    else:",
                    "        words = [w.split(\"-\")[0] for w in heb.words(f,speaker=speaker,replace=True,stem=True)]",
                    "        fout.write(str(convert_age_to_days(heb.age(f)[0])) + \"\\n\")",
                    "        for word in words:",
                    "            if(word != \"xxx\"):",
                    "                fout.write(word.encode('utf8') + \"\\n\")",
                    "    fout.close()",
                    "",
                    "def make_dir(directory):",
                    "    # Inputs a directory path",
                    "    # Creates a directory path at the input path if it does not already exist",
                    "    if not os.path.exists(directory):",
                    "        os.makedirs(directory)",
                    "",
                    "def output_words_to_files(child_files_dict):",
                    "    # Inputs dictionary from children to their files",
                    "    # Writes (for every child and every file) a list of words (with age on top) corresponding to that child and file",
                    "    # \"/[child_name]/[speaker][_stem?]/[child_name][0-9][a-c]\" where terminating characters come from original filename",
                    "    for child in child_files_dict.keys():",
                    "        directory = data_directory + child + \"/\"",
                    "        make_dir(directory)",
                    "        for f in child_files_dict[child]:",
                    "            child_dir = directory + \"child/\"",
                    "            mother_dir = directory + \"mother/\"",
                    "            child_stem_dir = directory + \"child_stem/\"",
                    "            mother_stem_dir = directory + \"mother_stem/\"",
                    "            file_name = (str(f)[11:-4].split(\"/\"))[1]",
                    "            make_dir(child_dir)",
                    "            make_dir(mother_dir)",
                    "            make_dir(child_stem_dir)",
                    "            make_dir(mother_stem_dir)",
                    "            write_list_to_file(f, child_dir + file_name, False, \"CHI\")",
                    "            write_list_to_file(f, mother_dir + file_name, False, \"MOT\")",
                    "            write_list_to_file(f, child_stem_dir + file_name, True, \"CHI\")",
                    "            write_list_to_file(f, mother_stem_dir + file_name, True, \"MOT\") ",
                    "            ",
                    "def write_sentences_to_file(f, file_path, speaker):",
                    "    # Inputs a file f, and out file_path file_path, and speaker (MOT or CHI)",
                    "    # Writes (to the file_path) sentences (one per line) used in f by speaker",
                    "    fout = open(file_path, \"w\")",
                    "    sentences = heb.sents(f, speaker=speaker,replace=True)",
                    "    for sentence in sentences:",
                    "        fout.write(\" \".join([word.encode('utf8') for word in sentence if word != \"xxx\"]) + \"\\n\")",
                    "    fout.close()",
                    "            ",
                    "def output_sentences_to_files(child_files_dict):",
                    "    # Inputs dictionary from children to their files",
                    "    # Writes (for every child and every file) a list of sentences corresponding to that child and file",
                    "    # \"/[child_name]/[speaker]_sentences/[child_name][0-9][a-c]\" where terminating characters come from original filename",
                    "    for child in child_files_dict.keys():",
                    "        directory = data_directory + child + \"/\"",
                    "        make_dir(directory)",
                    "        for f in child_files_dict[child]:",
                    "            child_sentence_dir = directory + \"child_sentences/\"",
                    "            mother_sentence_dir = directory + \"mother_sentences/\"",
                    "            file_name = (str(f)[11:-4].split(\"/\"))[1]",
                    "            make_dir(child_sentence_dir)",
                    "            make_dir(mother_sentence_dir)",
                    "            write_sentences_to_file(f, child_sentence_dir + file_name, \"CHI\")",
                    "            write_sentences_to_file(f, mother_sentence_dir + file_name, \"MOT\")",
                    "",
                    "# <10 min to run",
                    "output_words_to_files(child_files_dict)",
                    "output_sentences_to_files(child_files_dict)"
                ]
            },
            "output": {
                "state": {},
                "selectedType": "Hidden",
                "pluginName": "IPython",
                "shellId": "E45A8F63109D4A4797D9041015895E72",
                "elapsedTime": 235776,
                "dataresult": [
                    "BermanLong/hagar/hag107a.xml",
                    "BermanLong/hagar/hag107b.xml",
                    "BermanLong/hagar/hag107c.xml",
                    "BermanLong/hagar/hag107d.xml",
                    "BermanLong/hagar/hag107e.xml",
                    "BermanLong/hagar/hag107f.xml",
                    "BermanLong/hagar/hag107g.xml",
                    "BermanLong/hagar/hag107h.xml",
                    "BermanLong/hagar/hag107i.xml",
                    "BermanLong/hagar/hag108a.xml",
                    "BermanLong/hagar/hag108b.xml",
                    "BermanLong/hagar/hag108c.xml",
                    "BermanLong/hagar/hag108d.xml",
                    "BermanLong/hagar/hag108e.xml",
                    "BermanLong/hagar/hag108f.xml",
                    "BermanLong/hagar/hag108g.xml",
                    "BermanLong/hagar/hag108h.xml",
                    "BermanLong/hagar/hag108i.xml",
                    "BermanLong/hagar/hag109a.xml",
                    "BermanLong/hagar/hag109b.xml",
                    "BermanLong/hagar/hag109c.xml",
                    "BermanLong/hagar/hag109d.xml",
                    "BermanLong/hagar/hag109e.xml",
                    "BermanLong/hagar/hag109f.xml",
                    "BermanLong/hagar/hag108j.xml",
                    "BermanLong/hagar/hag109g.xml",
                    "BermanLong/hagar/hag109h.xml",
                    "BermanLong/hagar/hag109i.xml",
                    "BermanLong/hagar/hag109j.xml",
                    "BermanLong/hagar/hag109k.xml",
                    "BermanLong/hagar/hag110a.xml",
                    "BermanLong/hagar/hag110b.xml",
                    "BermanLong/hagar/hag110c.xml",
                    "BermanLong/hagar/hag110d.xml",
                    "BermanLong/hagar/hag110e.xml",
                    "BermanLong/hagar/hag110f.xml",
                    "BermanLong/hagar/hag110g.xml",
                    "BermanLong/hagar/hag110h.xml",
                    "BermanLong/hagar/hag110i.xml",
                    "BermanLong/hagar/hag110j.xml",
                    "BermanLong/hagar/hag110k.xml",
                    "BermanLong/hagar/hag110l.xml",
                    "BermanLong/hagar/hag110m.xml",
                    "BermanLong/hagar/hag110n.xml",
                    "BermanLong/hagar/hag110o.xml",
                    "BermanLong/hagar/hag110p1.xml",
                    "BermanLong/hagar/hag110p2.xml",
                    "BermanLong/hagar/hag110q.xml",
                    "BermanLong/hagar/hag111a.xml",
                    "BermanLong/hagar/hag111b.xml",
                    "BermanLong/hagar/hag111c.xml",
                    "BermanLong/hagar/hag111d.xml",
                    "BermanLong/hagar/hag111e.xml",
                    "BermanLong/hagar/hag111f.xml",
                    "BermanLong/hagar/hag111g.xml",
                    "BermanLong/hagar/hag111h.xml",
                    "BermanLong/hagar/hag111i.xml",
                    "BermanLong/hagar/hag111j1.xml",
                    "BermanLong/hagar/hag111j2.xml",
                    "BermanLong/hagar/hag111k.xml",
                    "BermanLong/hagar/hag200a.xml",
                    "BermanLong/hagar/hag200b.xml",
                    "BermanLong/hagar/hag200c.xml",
                    "BermanLong/hagar/hag200d.xml",
                    "BermanLong/hagar/hag201a.xml",
                    "BermanLong/hagar/hag201b.xml",
                    "BermanLong/hagar/hag201c1.xml",
                    "BermanLong/hagar/hag201c2.xml",
                    "BermanLong/hagar/hag201d.xml",
                    "BermanLong/hagar/hag201e.xml",
                    "BermanLong/hagar/hag201f.xml",
                    "BermanLong/hagar/hag201g.xml",
                    "BermanLong/hagar/hag201h.xml",
                    "BermanLong/hagar/hag201i.xml",
                    "BermanLong/hagar/hag201j.xml",
                    "BermanLong/hagar/hag201k.xml",
                    "BermanLong/hagar/hag201l.xml",
                    "BermanLong/hagar/hag201m1.xml",
                    "BermanLong/hagar/hag201m2.xml",
                    "BermanLong/hagar/hag202a.xml",
                    "BermanLong/hagar/hag202b.xml",
                    "BermanLong/hagar/hag202c.xml",
                    "BermanLong/hagar/hag202d.xml",
                    "BermanLong/hagar/hag202e.xml",
                    "BermanLong/hagar/hag202f.xml",
                    "BermanLong/hagar/hag202g.xml",
                    "BermanLong/hagar/hag202h.xml",
                    "BermanLong/hagar/hag202i.xml",
                    "BermanLong/hagar/hag202j.xml",
                    "BermanLong/hagar/hag203a.xml",
                    "BermanLong/hagar/hag203b.xml",
                    "BermanLong/hagar/hag203c.xml",
                    "BermanLong/hagar/hag203d.xml",
                    "BermanLong/hagar/hag203e.xml",
                    "BermanLong/hagar/hag204a.xml",
                    "BermanLong/hagar/hag204b.xml",
                    "BermanLong/hagar/hag204c.xml",
                    "BermanLong/hagar/hag204d.xml",
                    "BermanLong/hagar/hag204e.xml",
                    "BermanLong/hagar/hag204f.xml",
                    "BermanLong/hagar/hag204g.xml",
                    "BermanLong/hagar/hag204h.xml",
                    "BermanLong/hagar/hag204i.xml",
                    "BermanLong/hagar/hag204j.xml",
                    "BermanLong/hagar/hag204k.xml",
                    "BermanLong/hagar/hag204l.xml",
                    "BermanLong/hagar/hag205a.xml",
                    "BermanLong/hagar/hag205b.xml",
                    "BermanLong/hagar/hag205c.xml",
                    "BermanLong/hagar/hag206a.xml",
                    "BermanLong/hagar/hag206b.xml",
                    "BermanLong/hagar/hag206c.xml",
                    "BermanLong/hagar/hag206e.xml",
                    "BermanLong/hagar/hag206f.xml",
                    "BermanLong/hagar/hag207a.xml",
                    "BermanLong/hagar/hag207b.xml",
                    "BermanLong/hagar/hag207c.xml",
                    "BermanLong/hagar/hag207d.xml",
                    "BermanLong/hagar/hag207e.xml",
                    "BermanLong/hagar/hag207g.xml",
                    "BermanLong/hagar/hag208a.xml",
                    "BermanLong/hagar/hag208b.xml",
                    "BermanLong/hagar/hag208c.xml",
                    "BermanLong/hagar/hag208d.xml",
                    "BermanLong/hagar/hag208e.xml",
                    "BermanLong/hagar/hag208f.xml",
                    "BermanLong/hagar/hag208g.xml",
                    "BermanLong/hagar/hag209a.xml",
                    "BermanLong/hagar/hag209b.xml",
                    "BermanLong/hagar/hag209c.xml",
                    "BermanLong/hagar/hag209d.xml",
                    "BermanLong/hagar/hag210a.xml",
                    "BermanLong/hagar/hag211a.xml",
                    "BermanLong/hagar/hag303a-gold.xml"
                ]
            },
            "evaluatorReader": true,
            "lineCount": 74
        },
        {
            "id": "codeLf0EKJ",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "def lines_from_single_file(file_name):",
                    "    # Inputs a file_name",
                    "    # Outputs a list of words in that file",
                    "    with open(file_name, \"r\") as fin:",
                    "        contents = fin.read().decode('utf8').splitlines()",
                    "    return contents",
                    "    ",
                    "def read_words_from_files(child_list, data_dir):",
                    "    # Inputs a list of child names and a directory to read from",
                    "    # Outputs dictionary with keys child names and values a dictionary ",
                    "    # with keys file path and values a list of words corresponding to that file",
                    "    # {child_name : {file_name : [word1, word2,...]} ...}",
                    "    corpus_words = {}",
                    "    stat_types = [\"child\", \"mother\", \"child_stem\", \"mother_stem\"]",
                    "    for child in child_list:",
                    "        corpus_words[child] = {}",
                    "        for stat_type in stat_types:",
                    "            path = data_dir + child + \"/\" + stat_type + \"/\"",
                    "            files = os.listdir(path)",
                    "            for f in files:",
                    "                single_words = lines_from_single_file(path + f)",
                    "                (corpus_words[child])[path + f] = single_words",
                    "    return corpus_words",
                    "",
                    "def lines_to_sentence_list(file_name):",
                    "    # Inputs a file name",
                    "    # Outputs a list of sentences (a list of words) corresponding to that file",
                    "    with open(file_name, \"r\") as fin:",
                    "        contents = fin.read().decode('utf8').splitlines()",
                    "    sentence_list = []",
                    "    for sentence in contents:",
                    "        x = (sentence.split(' '))",
                    "        sentence_list.append(x)",
                    "#     print sentence_list # ASK MOSCOSO ABOUT WORD CONTRACTIONS, UNDERSCORES",
                    "    return sentence_list",
                    "",
                    "def read_sentences_from_files(child_list, data_dir):",
                    "    # Inputs a list of child names and a data directory",
                    "    # Outputs dictionary with keys child names and value dictionary ",
                    "    # with keys file path and values list of sentences corresponding to that file",
                    "    sentences = {}",
                    "    stat_types = [\"child_sentences\", \"mother_sentences\"]",
                    "    for child in child_list:",
                    "        sentences[child] = {}",
                    "        for stat_type in stat_types:",
                    "            path = data_dir + child + \"/\" + stat_type + \"/\"",
                    "            files = os.listdir(path)",
                    "            for f in files:",
                    "                single_sentences = lines_to_sentence_list(path + f)",
                    "                (sentences[child])[path + f] = single_sentences",
                    "    return sentences",
                    "",
                    "# 10 seconds to run",
                    "child_list = child_files_dict.keys()",
                    "corpus_words = read_words_from_files(child_list, data_directory)",
                    "sentences = read_sentences_from_files(child_list, data_directory)"
                ]
            },
            "output": {
                "state": {},
                "selectedType": "BeakerDisplay",
                "pluginName": "IPython",
                "shellId": "E45A8F63109D4A4797D9041015895E72",
                "elapsedTime": 1501
            },
            "evaluatorReader": true,
            "lineCount": 56
        },
        {
            "id": "code2CHrOj",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "def get_mean_length_utterances(window_files, child):",
                    "    # Syntactic Diversity",
                    "    # Inputs a window of files and a child name",
                    "    # Outputs the MLU of the child and mother in that window of files",
                    "    child_sents = []",
                    "    mother_sents = []",
                    "    for window_file in window_files:",
                    "        child_sents += sentences[child][data_directory + child + \"/\" + \"child_sentences\" + \"/\" + window_file]",
                    "        mother_sents += sentences[child][data_directory + child + \"/\" + \"mother_sentences\" + \"/\" + window_file]",
                    "    child_len_words = [] ",
                    "    for i in range(len(child_sents)):",
                    "        child_len_words.append(len(child_sents[i]))",
                    "    child_MLU = np.mean(child_len_words)",
                    "    mother_len_words = [] ",
                    "    for i in range(len(mother_sents)):",
                    "        mother_len_words.append(len(mother_sents[i]))",
                    "    mother_MLU = np.mean(mother_len_words)",
                    "    return child_MLU, mother_MLU",
                    "",
                    "def make_freq_dist_files(window_files, child, use_stem):",
                    "    # Inputs a window of files, a child name, and a stem flag",
                    "    # Outputs a Freq dist for the both the child and mother (stemmed if specified)",
                    "    child_words = []",
                    "    mother_words = []",
                    "    if(use_stem):",
                    "        for window_file in window_files:",
                    "            # Remove age from words",
                    "            child_words += (corpus_words[child][data_directory + child + '/child_stem/' + window_file])[1:]",
                    "            mother_words += (corpus_words[child][data_directory + child + '/mother_stem/' + window_file])[1:]",
                    "    else:",
                    "        for window_file in window_files:",
                    "            # Remove age from words",
                    "            child_words += (corpus_words[child][data_directory + child + '/child/' + window_file])[1:]",
                    "            mother_words += (corpus_words[child][data_directory + child + '/mother/' + window_file])[1:]",
                    "    return FreqDist(child_words), FreqDist(mother_words)",
                    "",
                    "def correct(list_files):",
                    "    # Inputs a list of files",
                    "    # Outputs \"anne01a\" type file names",
                    "    correct_file_names = []",
                    "    for i in range(len(list_files)):",
                    "        correct_file_names.append((((list_files[i])[11:-4]).split(\"/\"))[1])",
                    "    return correct_file_names",
                    "",
                    "def make_windows(window_size, child):",
                    "    # Inputs a window size of child and a child name",
                    "    # Outputs a list of window_size window_files corresponding to that child",
                    "    list_window_files = []",
                    "    if len(child_files_dict[child]) < window_size:",
                    "        # Windows cannot be larger than the number of files per child",
                    "        raise ValueError(\"Requested larger window than number of files.\") ",
                    "    else:",
                    "        i = 0",
                    "        while(i + window_size <= len(child_files_dict[child])):",
                    "            correct_file_names = correct(child_files_dict[child][i:i + window_size])",
                    "            list_window_files.append(correct_file_names)",
                    "            i += 1",
                    "    return list_window_files",
                    "",
                    "def window_to_weighted_age(window_files, child):",
                    "    # Inputs a single window file of child and a child name",
                    "    # Outputs its weighted (by number of words) age in that window",
                    "    weighted_age = 0.0",
                    "    number_of_words = []",
                    "    for window_file in window_files:",
                    "        data = corpus_words[child][data_directory + child + '/child/' + window_file]",
                    "        n = sum(array((FreqDist(data)).values()))",
                    "        number_of_words.append(n)",
                    "        window_age = int(data[0]) * n",
                    "        weighted_age += window_age",
                    "    weighted_age /= sum(number_of_words) ",
                    "    return weighted_age"
                ]
            },
            "output": {
                "state": {},
                "selectedType": "BeakerDisplay",
                "pluginName": "IPython",
                "shellId": "E45A8F63109D4A4797D9041015895E72",
                "elapsedTime": 372
            },
            "evaluatorReader": true,
            "lineCount": 72
        },
        {
            "id": "codeBiZG51",
            "type": "code",
            "evaluator": "IPython",
            "input": {
                "body": [
                    "import time",
                    "",
                    "start = time.time()",
                    "window_size = 3",
                    "fout = open(nmmfile,\"w\")",
                    "print >> fout, \"Child Age N.child H.child.S H.child.I Schild N.mother H.mother.S H.mother.I Smother\"",
                    "for child in child_list:",
                    "    for window in make_windows(window_size, child):",
                    "        # Age",
                    "        age = window_to_weighted_age(window, child)",
                    "        # Freq Dists",
                    "        fchild, fmother = make_freq_dist_files(window, child, False)",
                    "        fchildS, fmotherS = make_freq_dist_files(window, child, True)",
                    "        # Statistics",
                    "        nchild = sum(array(fchild.values()))",
                    "        nmother = sum(array(fmother.values()))",
                    "        # Entropies",
                    "        Hchild = Ent.Entropy(fchild,method=\"CWJ\")",
                    "        Hmother = Ent.Entropy(fmother,method=\"CWJ\")",
                    "        # Entropies (stemmed - Lexical Diversity)",
                    "        HchildS = Ent.Entropy(fchildS,method=\"CWJ\")",
                    "        HmotherS = Ent.Entropy(fmotherS,method=\"CWJ\")",
                    "        # Inflectional Diversity",
                    "        HchildI = Hchild - HchildS",
                    "        HmotherI = Hmother - HmotherS",
                    "        # Syntactic Diversity (MLU) BOTTLENECK",
                    "        Schild, Smother = get_mean_length_utterances(window, child)",
                    "        ",
                    "#         print child,age,nchild,HchildS,HchildI,Schild,nmother,HmotherS,HmotherI,Smother",
                    "        print >> fout, child,age,nchild,HchildS,HchildI,Schild,nmother,HmotherS,HmotherI,Smother",
                    "",
                    "fout.close()",
                    "print \"Runtime for window_size \" + str(window_size) + \" is \" + str(time.time() - start)",
                    "# 18s to run for w_size = 3"
                ]
            },
            "output": {
                "state": {},
                "result": {
                    "type": "Results",
                    "outputdata": [
                        {
                            "type": "out",
                            "value": "Runtime for window_size 3 is 3.33967018127\n"
                        },
                        {
                            "type": "err",
                            "value": "/Library/Python/2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
                        }
                    ]
                },
                "selectedType": "Results",
                "pluginName": "IPython",
                "shellId": "E45A8F63109D4A4797D9041015895E72",
                "elapsedTime": 3563
            },
            "evaluatorReader": true,
            "lineCount": 34
        }
    ],
    "namespace": {}
}
